{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHcn4VafE1WJ"
   },
   "source": [
    "## Ortega Forward Propagating Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation for a neural network training on an MNIST model using python lists. Although not practical at all for real life, it was a nice exercise in combining knowledge of data structures and algorithms and incorporating this knowledge into the field of Machine Learning. I had a blast :) . Weights and Hidden layers from a fully trained Tensorflow model and fed as either weights for the model or for testing purposes by comparing with the outputs of the hidden layers of my Forward Propagating Neural Network. At a future point, including back-propagation to incorporate a fully-trainable model are not out of the picture as it would only require backwards functionality for the 2 layer classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j9KZgxP6GxV4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from iteration_utilities import deepflatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_nlduo9stvq"
   },
   "source": [
    "## Tensor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9QNhWLbRe-aN"
   },
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    def __init__(self, data, shape):\n",
    "        self.shape = shape\n",
    "        self.data = data\n",
    "        self.tensor = self.shape_tensor() if shape else []\n",
    "        \n",
    "    @property\n",
    "    def transpose(self):\n",
    "        tensor = Tensor([], self.shape[:-2] + [self.shape[-1], self.shape[-2]])\n",
    "        if len(self.shape) == 2:\n",
    "            for x in range(self.shape[0]):\n",
    "                for y in range(self.shape[1]):\n",
    "                    tensor.tensor[y][x] = self.tensor[x][y]\n",
    "        else:\n",
    "            for i in range(self.num_entries(is_batch=True)):\n",
    "                t = tensor.get_entry(tensor.entry_loc(i, is_batch=True))\n",
    "                s = self.get_entry(self.entry_loc(i, is_batch=True))\n",
    "                for x in range(self.shape[-2]):\n",
    "                    for y in range(self.shape[-1]):\n",
    "                        t[y][x] = s[x][y]\n",
    "        return tensor\n",
    "    \n",
    "    def num_entries(self, is_batch=False) : \n",
    "        num_entries = 1\n",
    "        dims = self.shape[:-2] if is_batch else self.shape\n",
    "        for dim in dims: \n",
    "             num_entries *= dim  \n",
    "        return num_entries\n",
    "    \n",
    "    def entry_loc(self, entry_num, is_batch=False):\n",
    "        entry_loc = []\n",
    "        dims = self.shape[:-2][::-1] if is_batch else self.shape[::-1]\n",
    "        for elem in dims:\n",
    "            entry_loc = [entry_num % elem] + entry_loc\n",
    "            entry_num  = entry_num // elem        \n",
    "        return entry_loc\n",
    "    \n",
    "    def shape_compatible(self, tensor2, op):\n",
    "        if len(self.shape) == len(tensor2.shape) and op == 'add':\n",
    "            zipped = zip(self.shape, tensor2.shape)\n",
    "        elif len(self.shape[:-2]) == len(tensor2.shape[:-2]) and op == 'matmul':\n",
    "            zipped = zip(self.shape[:-2], tensor2.shape[:-2]) \n",
    "        else:\n",
    "            return False\n",
    "        return all(x == y for x,y in zipped)\n",
    "\n",
    "    def shape_broadcastable(self, tensor2):\n",
    "        if len(tensor2.shape) == 2 and self.shape[-1] == tensor2.shape[1] \\\n",
    "           and tensor2.shape[0] == 1:\n",
    "            return True\n",
    "        elif len(tensor2.shape) == 1 and self.shape[-1] == tensor2.shape[0]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    " \n",
    "    def set_entry(self, entry_loc, entry):\n",
    "        def set_helper(tensor, entry_loc, entry):\n",
    "            if len(entry_loc) == 1:\n",
    "                tensor[entry_loc[0]] = entry\n",
    "            else:\n",
    "                set_helper(tensor[entry_loc[0]], entry_loc[1:], entry)\n",
    "        \n",
    "        set_helper(self.tensor, entry_loc, entry)\n",
    "        \n",
    "    def get_entry(self, entry_loc):\n",
    "        def get_helper(tensor, entry_loc):\n",
    "            if len(entry_loc) == 1:\n",
    "                return tensor[entry_loc[0]]\n",
    "            else:\n",
    "                return get_helper(tensor[entry_loc[0]], entry_loc[1:])\n",
    "        \n",
    "        return get_helper(self.tensor, entry_loc)\n",
    " \n",
    "    def shape_tensor(self):\n",
    "        assert all(isinstance(dim, int) and dim > 0 for dim in self.shape)\n",
    "        assert all(isinstance(pt, int) or isinstance(pt, float) for pt in self.data)\n",
    "        \n",
    "        def build_tensor(data, shape):\n",
    "            return [build_tensor(data, shape[1:]) for i in range(shape[0])] if shape else 0.0\n",
    "            \n",
    "        self.tensor = build_tensor(self.data, self.shape)\n",
    "        \n",
    "        if self.tensor:\n",
    "            num_entries = min(len(self.data), self.num_entries(is_batch=False))\n",
    "            for i in range(num_entries):\n",
    "                self.set_entry(self.entry_loc(i), self.data[i]) \n",
    "        return self.tensor\n",
    "\n",
    "    def apply(self, op):\n",
    "        sigmoid = lambda x: 1 / (1 + (math.e ** (-1 * x)))\n",
    "        tanh = lambda x: ((math.e ** x) - math.e ** (-1 * x)) / ((math.e ** x) + math.e ** (-1 * x))\n",
    "        relu = lambda x: x if x > 0 else 0\n",
    "        tensor = Tensor([], self.shape)\n",
    "\n",
    "        for i in range(self.num_entries()):\n",
    "            x = self.get_entry(self.entry_loc(i))\n",
    "            if op == 'sigmoid':\n",
    "                tensor.set_entry(tensor.entry_loc(i), sigmoid(x))\n",
    "            elif op == 'tanh':\n",
    "                tensor.set_entry(tensor.entry_loc(i), tanh(x))\n",
    "            else:\n",
    "                tensor.set_entry(tensor.entry_loc(i), relu(x))\n",
    "\n",
    "        return tensor\n",
    "            \n",
    "    \n",
    "    def add(self, tensor2):\n",
    "        \n",
    "        tensor = Tensor([], self.shape)\n",
    "        if self.shape_compatible(tensor2, 'add'):\n",
    "            for i in range(self.num_entries()):\n",
    "                total = self.get_entry(self.entry_loc(i)) + \\\n",
    "                        tensor2.get_entry(tensor.entry_loc(i))\n",
    "                tensor.set_entry(self.entry_loc(i), total)   \n",
    "        elif self.shape_broadcastable(tensor2):\n",
    "            n = tensor2.shape[-1]\n",
    "            for i in range(self.num_entries()):\n",
    "                total = self.get_entry(self.entry_loc(i)) + \\\n",
    "                        tensor2.get_entry(tensor2.entry_loc(i % n))\n",
    "                tensor.set_entry(tensor.entry_loc(i), total)\n",
    "        else:\n",
    "            raise ValueError(f'''incompatible shapes for add: t1.shape {self.shape}, t2.shape {tensor2.shape}''')\n",
    "        \n",
    "        return tensor\n",
    "  \n",
    "    def dot_product(self, tensor1, tensor2):\n",
    "        total = 0\n",
    "        \n",
    "        for x,y in zip(tensor1, tensor2):\n",
    "            total += x * y\n",
    "        return total\n",
    "    \n",
    "    def matmul(self, tensor2):   \n",
    "        if self.shape_compatible(tensor2, 'matmul'):\n",
    "            t = Tensor([], self.shape[:-2] + [self.shape[-2], tensor2.shape[-1]])\n",
    "            tensor2 = tensor2.transpose\n",
    "            \n",
    "            def matmul_helper(tensor1, tensor2):\n",
    "                nonlocal current_batch\n",
    "                nonlocal t\n",
    "                for x in range(t.shape[-2]):\n",
    "                    for y in range(t.shape[-1]):\n",
    "                        current_batch[x][y] = self.dot_product(tensor1[x], tensor2[y])\n",
    "                \n",
    "            if len(t.shape) > 2:\n",
    "                for i in range(t.num_entries(is_batch=True)):\n",
    "                    current_batch = t.get_entry(t.entry_loc(i, is_batch=True))\n",
    "                    t1 = self.get_entry(self.entry_loc(i, is_batch=True))\n",
    "                    t2 = tensor2.get_entry(tensor2.entry_loc(i, is_batch=True))\n",
    "                    matmul_helper(t1, t2)\n",
    "            else:\n",
    "                current_batch = t.tensor\n",
    "                matmul_helper(self.tensor, tensor2.tensor)\n",
    "                \n",
    "            tensor2 = tensor2.transpose  \n",
    "            return t\n",
    "   \n",
    "        else:\n",
    "            raise ValueError(f'''incompatible shapes for matmul: t1.shape {self.shape}, t2.shape {tensor2.shape}''') \n",
    "            \n",
    "    def softmax(self):\n",
    "        if len(self.shape) == 2:\n",
    "            num_points, num_classes = self.shape[0], self.shape[1]\n",
    "            t = Tensor(self.data, [num_points, num_classes])\n",
    "            for pt in range(num_points):\n",
    "                \n",
    "                normalization = 0\n",
    "                max_so_far = float('-inf')\n",
    "                for c in range(num_classes):\n",
    "                    if max_so_far < self.tensor[pt][c]:\n",
    "                         max_so_far = self.tensor[pt][c]\n",
    "                \n",
    "                for c in range(num_classes): \n",
    "                    t.tensor[pt][c] = math.e ** (self.tensor[pt][c] - max_so_far)\n",
    "                    normalization += t.tensor[pt][c]\n",
    "                \n",
    "                for c in range(num_classes):\n",
    "                    t.tensor[pt][c] /= normalization\n",
    "        else:\n",
    "            raise ValueError(f'''incompatible shape for softmax: shape {self.shape}''')\n",
    "        return t\n",
    "\n",
    "    def argmax(self):\n",
    "        if len(self.shape) == 2:\n",
    "            num_points, num_classes = self.shape[0], self.shape[1]\n",
    "            t = Tensor([], [num_points])\n",
    "            for pt in range(num_points):\n",
    "                \n",
    "                max_c, max_prob   = 0, 0\n",
    "                for c in range(num_classes):\n",
    "                    if max_prob < self.tensor[pt][c]:\n",
    "                        max_c = c\n",
    "                        max_prob = self.tensor[pt][c]\n",
    "                t.tensor[pt] = max_c        \n",
    "        else:\n",
    "            raise ValueError(f'''incompatible shape for softmax: shape {self.shape}''')\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Classes (Dense Layer + Activation Layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(): \n",
    "    def __init__(self, data, weights, bias):\n",
    "        self.data = data\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.data.matmul(self.weights).add(self.bias)\n",
    "\n",
    "\n",
    "class Activation():\n",
    "    def __init__(self, data,  activation):\n",
    "        valid_ops = {'sigmoid', 'tanh', 'relu'}\n",
    "        self.data = data\n",
    "        if activation not in valid_ops:\n",
    "             raise ValueError(f'''invalid activation fn: {op} not in {valid_ops}''') \n",
    "        self.activation = activation\n",
    "        \n",
    "    \n",
    "    def forward(self):\n",
    "        return self.data.apply(self.activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Model():\n",
    "    def __init__(self, weights):\n",
    "        self.weights = weights\n",
    "        self.tensors = None\n",
    "    \n",
    "    def predict(self, data):\n",
    "        if not (self.tensors \\\n",
    "            and self.tensors['predict'].shape[0] == data.shape[0]):\n",
    "            self.run(data)\n",
    "        return self.tensors['predict']\n",
    "        \n",
    "    \n",
    "    def run(self, data):\n",
    "        self.tensors = {}\n",
    "        self.tensors['l0'] = np.array(data.tensor)\n",
    "        layer1 = Dense(x_test, self.weights['w1'], self.weights['b1']).forward()\n",
    "        act1 = Activation(layer1, 'sigmoid').forward()\n",
    "        self.tensors['l1'] = np.array(act1.tensor)\n",
    "        print('l1 done')\n",
    "        layer2 = Dense(act1, self.weights['w2'], self.weights['b2']).forward()\n",
    "        act2 = Activation(layer2, 'sigmoid').forward()\n",
    "        self.tensors['l2'] = np.array(act2.tensor)\n",
    "        print('l2 done')\n",
    "        layer3 = Dense(act2, self.weights['w3'], self.weights['b3']).forward()\n",
    "        act3 = Activation(layer3, 'sigmoid').forward()\n",
    "        self.tensors['l3'] = np.array(act3.tensor)\n",
    "        print('l3 done')\n",
    "        layer4 = Dense(act3, self.weights['w4'], self.weights['b4']).forward()\n",
    "        act4 = Activation(layer4, 'sigmoid').forward()\n",
    "        self.tensors['l4'] = np.array(act4.tensor)\n",
    "        print('l4 done')\n",
    "        softmax = act4.softmax()\n",
    "        self.tensors['softmax'] = np.array(softmax.tensor)\n",
    "        print('softmax done')\n",
    "        preds = softmax.argmax()\n",
    "        self.tensors['predict'] = np.array(preds.tensor)\n",
    "        print('argmax done')\n",
    "        return self.tensors\n",
    "    \n",
    "    def accuracy(self, y_pred, y):\n",
    "        correct_pred = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y[i]:\n",
    "                correct_pred += 1\n",
    "        print('correct preds: ', correct_pred)\n",
    "        return correct_pred / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtJe6553s3g_"
   },
   "source": [
    "### Loading Weight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6lhs7M8XeCk4"
   },
   "outputs": [],
   "source": [
    "model_weights = {}\n",
    "names = ['w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4']\n",
    "for name in names:\n",
    "    file = open(f'weights/{name}.txt', 'rb')\n",
    "    model_weights[name] = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR6r8Juora8o",
    "outputId": "39a4c280-b9f4-4912-c0ae-c4bc035c943f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (784, 784)\n",
      "b1 (784,)\n",
      "w2 (784, 200)\n",
      "b2 (200,)\n",
      "w3 (200, 20)\n",
      "b3 (20,)\n",
      "w4 (20, 10)\n",
      "b4 (10,)\n"
     ]
    }
   ],
   "source": [
    "for name in model_weights.keys():\n",
    "    print(name, model_weights[name].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmdXhuCTMofO"
   },
   "source": [
    "### Loading Model Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "guk-BTq6Hbin"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train.reshape((60000, 784))\n",
    "x_test = x_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seE-2G6wuH2u",
    "outputId": "961e8440-d171-4a6b-c1ab-76a818b130d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eHajzmt6SPq"
   },
   "source": [
    "### Pre-process weights/data for Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ums5Xbq06YEw",
    "outputId": "f6865b16-dbe3-48da-8195-cad0970ed5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Tensor object at 0x00000139F5650220>\n",
      "<__main__.Tensor object at 0x00000139BE0DA670>\n",
      "<__main__.Tensor object at 0x00000139F56398B0>\n",
      "<__main__.Tensor object at 0x00000139BE0DAA30>\n",
      "<__main__.Tensor object at 0x00000139BFA7FF40>\n",
      "<__main__.Tensor object at 0x00000139BE0C0E50>\n",
      "<__main__.Tensor object at 0x00000139BFA7F3A0>\n",
      "<__main__.Tensor object at 0x00000139BFA7FB20>\n"
     ]
    }
   ],
   "source": [
    "for name in model_weights.keys():\n",
    "    model_weights[name] = Tensor(list(deepflatten(model_weights[name].tolist())), \n",
    "                                 list(model_weights[name].shape))\n",
    "    print(model_weights[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCgu27KUoAS5",
    "outputId": "5794e537-869a-4996-9e04-0f0ae9fc1c11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60000, 784]\n",
      "[60000]\n",
      "[10000, 784]\n",
      "[10000]\n"
     ]
    }
   ],
   "source": [
    "x_train = Tensor(list(deepflatten(x_train.tolist())), list(x_train.shape))\n",
    "print(x_train.shape)\n",
    "y_train = Tensor(y_train.tolist(), list(y_train.shape))\n",
    "print(y_train.shape)\n",
    "x_test = Tensor(list(deepflatten(x_test.tolist())), list(x_test.shape))\n",
    "print(x_test.shape)\n",
    "y_test = Tensor(y_test.tolist(), list(y_test.shape))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXCq2iJL0sTt"
   },
   "source": [
    "### Testing Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pMtO0LJ8wDhm"
   },
   "outputs": [],
   "source": [
    "data = [i for i in range(-1, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6F_MG8Yj0zkv"
   },
   "outputs": [],
   "source": [
    "s_t = Tensor(data, [3])\n",
    "t_t = Tensor(data, [3])\n",
    "r_t = Tensor(data, [3])\n",
    "\n",
    "a_1 = Activation(s_t, 'sigmoid')\n",
    "a_2 = Activation(t_t, 'tanh')\n",
    "a_3 = Activation(r_t, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c8cAiZZ1U_C",
    "outputId": "9f874ade-8519-4edf-a75d-ca80816b60d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2689414213699951, 0.5, 0.7310585786300049]\n",
      "[-0.7615941559557649, 0.0, 0.7615941559557649]\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(a_1.forward().tensor)\n",
    "print(a_2.forward().tensor)\n",
    "print(a_3.forward().tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jusIOb87moiF"
   },
   "source": [
    "### Testing Outputs of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5E0mGl2QUYv",
    "outputId": "80a0b6a0-4dbd-4e9b-b08e-94027f0cb9cc"
   },
   "outputs": [],
   "source": [
    "m = MNIST_Model(model_weights)\n",
    "my_hidden_layers = m.run(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPOAwxgjUysQ"
   },
   "outputs": [],
   "source": [
    "model_layers = {}\n",
    "names = ['l0', 'l1', 'l2', 'l3', 'l4', 'softmax', 'predict']\n",
    "for name in names:\n",
    "    file = open(f'hidden_layer/{name}.txt', 'rb')\n",
    "    model_layers[name] = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxVpE3TucHvB",
    "outputId": "17663451-2a7e-429a-92a8-e15f1d2486f0"
   },
   "outputs": [],
   "source": [
    "for name in model_layers.keys():\n",
    "    my_layer = my_hidden_layers[name]\n",
    "    test_layer = model_layers[name]\n",
    "    if name == 'predict':\n",
    "        test_layer = test_layer.argmax(axis=1)\n",
    "    print(my_layer.shape, test_layer.shape, 'shapes')\n",
    "    print(f'''output error for {name}:''', np.sum(np.abs(my_layer - test_layer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZCslAuav6Zl"
   },
   "source": [
    "### Verifying Model Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvydrlrYv5Nb",
    "outputId": "751f98fc-c89e-4086-f233-59378c36fc2b"
   },
   "outputs": [],
   "source": [
    "print(f'''this is my model accuracy {m.accuracy(m.tensors['predict'], y_test.tensor)}''')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dense Layer Implementation: Part 2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
